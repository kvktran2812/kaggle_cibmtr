{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b677db92-6c28-4a7e-8e35-69c75db12d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from lifelines.utils import concordance_index\n",
    "from metric import score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ae3b70-37f0-4686-a8d0-1522cbfdcb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train.csv\"\n",
    "test_path = \"data/test.csv\"\n",
    "sample_path = \"data/sample_submission.csv\"\n",
    "data_dict_path = \"data/data_dictionary.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "data_dict_df = pd.read_csv(data_dict_path)\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "def transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(df[time_col], df[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).values\n",
    "    return y\n",
    "train[\"y\"] = transform_survival_probability(train, time_col='efs_time', event_col='efs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db972f64-2d1a-4658-bcb8-b4f295321eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n",
    "FEATURES = [c for c in train.columns if not c in RMV]\n",
    "# print(f\"Total features: {len(FEATURES)} - {FEATURES}\")\n",
    "\n",
    "CATS = []\n",
    "NULLS = [\"Not done\", \"Not tested\", \"N/A\", \"N/A, Mel not given\", \"No drugs reported\"]\n",
    "NUMS = []\n",
    "\n",
    "for c in FEATURES:\n",
    "    if train[c].dtype==\"object\":\n",
    "        for null in NULLS:\n",
    "            if null in train[c].unique():\n",
    "                train[c] = train[c].fillna(null)\n",
    "                test[c] = test[c].fillna(null)\n",
    "        train[c] = train[c].fillna(\"NAN\")\n",
    "        test[c] = test[c].fillna(\"NAN\")\n",
    "\n",
    "        CATS.append(c)\n",
    "    else:\n",
    "        NUMS.append(c)\n",
    "        train[c] = train[c].fillna(-1)\n",
    "        test[c] = test[c].fillna(-1)\n",
    "\n",
    "CAT_SIZE = []\n",
    "CAT_EMB = []\n",
    "NUMS = []\n",
    "\n",
    "combined = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "#print(\"Combined data shape:\", combined.shape )\n",
    "\n",
    "# print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \")\n",
    "\n",
    "for c in FEATURES:\n",
    "    if c in CATS:\n",
    "        # LABEL ENCODE\n",
    "        combined[c],_ = combined[c].factorize()\n",
    "        combined[c] -= combined[c].min()\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "        #combined[c] = combined[c].astype(\"category\")\n",
    "\n",
    "        n = combined[c].nunique()\n",
    "        mn = combined[c].min()\n",
    "        mx = combined[c].max()\n",
    "        # print(f'{c} has ({n}) unique values')\n",
    "\n",
    "        CAT_SIZE.append(mx+1) \n",
    "        CAT_EMB.append( int(np.ceil( np.sqrt(mx+1))) ) \n",
    "    else:\n",
    "        if combined[c].dtype==\"float64\":\n",
    "            combined[c] = combined[c].astype(\"float32\")\n",
    "        if combined[c].dtype==\"int64\":\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "            \n",
    "        m = combined[c].mean()\n",
    "        s = combined[c].std()\n",
    "        combined[c] = (combined[c]-m)/s\n",
    "        combined[c] = combined[c].fillna(0)\n",
    "        \n",
    "        NUMS.append(c)\n",
    "        \n",
    "train = combined.iloc[:len(train)].copy()\n",
    "test = combined.iloc[len(train):].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab09a943-4953-4dce-8a03-8822fd7504dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd86e19-49b7-4494-8410-7c09d4ce78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d594b9cc-bcbb-40e4-89f2-2f065a8a9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, cat_sizes, cat_emb_sizes, num_features):\n",
    "        super(SurvivalModel, self).__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(size, emb_size) \n",
    "                                         for size, emb_size in zip(cat_sizes, cat_emb_sizes)])\n",
    "        self.num_features = num_features\n",
    "        total_emb_size = sum(cat_emb_sizes) + num_features\n",
    "        \n",
    "        self.fc1 = nn.Linear(total_emb_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x_cat, x_num):\n",
    "        embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat(embs + [x_num], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e225f92-65b2-476f-967b-0127a4ec0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, optimizer, criterion, device, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x_cat, x_num, y in train_loader:\n",
    "            x_cat, x_num, y = x_cat.to(device), x_num.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_cat, x_num)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_cat, x_num, y in valid_loader:\n",
    "                x_cat, x_num, y = x_cat.to(device), x_num.to(device), y.to(device)\n",
    "                output = model(x_cat, x_num)\n",
    "                valid_loss += criterion(output, y).item()\n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {valid_loss/len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce285862-5d3b-49d0-b562-07ced568bcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617c0470-78af-44b2-a04d-2dfc8a9b7188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### REPEAT 1 ###\n",
      "### Fold 1 ###\n",
      "Epoch 1, Validation Loss: 0.028405260915557545\n",
      "Epoch 2, Validation Loss: 0.027676138716439407\n",
      "Epoch 3, Validation Loss: 0.02714888487632076\n",
      "Epoch 4, Validation Loss: 0.026715842851748068\n",
      "### Fold 2 ###\n",
      "Epoch 1, Validation Loss: 0.02892059072231253\n",
      "Epoch 2, Validation Loss: 0.02791445019344489\n",
      "Epoch 3, Validation Loss: 0.027547708712518215\n",
      "Epoch 4, Validation Loss: 0.027293898010005552\n",
      "### Fold 3 ###\n",
      "Epoch 1, Validation Loss: 0.02919255414356788\n",
      "Epoch 2, Validation Loss: 0.027525535163780052\n",
      "Epoch 3, Validation Loss: 0.0267950890896221\n",
      "Epoch 4, Validation Loss: 0.026592797910173733\n",
      "### Fold 4 ###\n",
      "Epoch 1, Validation Loss: 0.02967660517121355\n",
      "Epoch 2, Validation Loss: 0.028026817521701258\n",
      "Epoch 3, Validation Loss: 0.027579618617892265\n",
      "Epoch 4, Validation Loss: 0.027319165257116158\n",
      "### Fold 5 ###\n",
      "Epoch 1, Validation Loss: 0.028768180248637993\n",
      "Epoch 2, Validation Loss: 0.02807827712967992\n",
      "Epoch 3, Validation Loss: 0.027537064782033365\n",
      "Epoch 4, Validation Loss: 0.027378728458036978\n",
      "### REPEAT 2 ###\n",
      "### Fold 1 ###\n",
      "Epoch 1, Validation Loss: 0.028654186986386776\n",
      "Epoch 2, Validation Loss: 0.028043331267933052\n",
      "Epoch 3, Validation Loss: 0.0278027366536359\n",
      "Epoch 4, Validation Loss: 0.027414599743982155\n",
      "### Fold 2 ###\n",
      "Epoch 1, Validation Loss: 0.02908602124080062\n",
      "Epoch 2, Validation Loss: 0.027726000640541315\n",
      "Epoch 3, Validation Loss: 0.027179276725898188\n",
      "Epoch 4, Validation Loss: 0.026875588111579418\n",
      "### Fold 3 ###\n",
      "Epoch 1, Validation Loss: 0.028854360648741324\n",
      "Epoch 2, Validation Loss: 0.027902722358703613\n",
      "Epoch 3, Validation Loss: 0.027310927243282396\n",
      "Epoch 4, Validation Loss: 0.02693046536296606\n",
      "### Fold 4 ###\n",
      "Epoch 1, Validation Loss: 0.029221934576829273\n",
      "Epoch 2, Validation Loss: 0.02754987558970849\n",
      "Epoch 3, Validation Loss: 0.027219236362725496\n",
      "Epoch 4, Validation Loss: 0.02690610211963455\n",
      "### Fold 5 ###\n",
      "Epoch 1, Validation Loss: 0.02849258715286851\n",
      "Epoch 2, Validation Loss: 0.02776540582999587\n",
      "Epoch 3, Validation Loss: 0.027295677457004786\n",
      "Epoch 4, Validation Loss: 0.02714773127809167\n",
      "### REPEAT 3 ###\n",
      "### Fold 1 ###\n",
      "Epoch 1, Validation Loss: 0.028398487406472366\n",
      "Epoch 2, Validation Loss: 0.027591060536603134\n",
      "Epoch 3, Validation Loss: 0.027212569179634254\n",
      "Epoch 4, Validation Loss: 0.02712292115514477\n",
      "### Fold 2 ###\n",
      "Epoch 1, Validation Loss: 0.029831556758532923\n",
      "Epoch 2, Validation Loss: 0.02856240188702941\n",
      "Epoch 3, Validation Loss: 0.027810218123098213\n",
      "Epoch 4, Validation Loss: 0.027614100836217403\n",
      "### Fold 3 ###\n",
      "Epoch 1, Validation Loss: 0.02919938958560427\n",
      "Epoch 2, Validation Loss: 0.027931783193101484\n",
      "Epoch 3, Validation Loss: 0.02749099163338542\n",
      "Epoch 4, Validation Loss: 0.027068751243253548\n",
      "### Fold 4 ###\n",
      "Epoch 1, Validation Loss: 0.029532686341553926\n",
      "Epoch 2, Validation Loss: 0.02770806849002838\n",
      "Epoch 3, Validation Loss: 0.027245001091311376\n",
      "Epoch 4, Validation Loss: 0.027013832548012335\n",
      "### Fold 5 ###\n",
      "Epoch 1, Validation Loss: 0.029529685775438946\n",
      "Epoch 2, Validation Loss: 0.028016654929767053\n",
      "Epoch 3, Validation Loss: 0.027672470236817997\n",
      "Epoch 4, Validation Loss: 0.02741941421603163\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "REPEATS = 3\n",
    "FOLDS = 5\n",
    "kf = KFold(n_splits=FOLDS, random_state=42, shuffle=True)\n",
    "\n",
    "oof_nn = np.zeros(len(train))\n",
    "pred_nn = np.zeros(len(test))\n",
    "\n",
    "for r in range(REPEATS):\n",
    "    print(f\"### REPEAT {r+1} ###\")\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "        print(f\"### Fold {i+1} ###\")\n",
    "        \n",
    "        X_train_cats = torch.tensor(train.loc[train_index, CATS].values, dtype=torch.long)\n",
    "        X_train_nums = torch.tensor(train.loc[train_index, NUMS].values, dtype=torch.float32)\n",
    "        y_train = torch.tensor(train.loc[train_index, \"y\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        X_valid_cats = torch.tensor(train.loc[test_index, CATS].values, dtype=torch.long)\n",
    "        X_valid_nums = torch.tensor(train.loc[test_index, NUMS].values, dtype=torch.float32)\n",
    "        y_valid = torch.tensor(train.loc[test_index, \"y\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_cats, X_train_nums, y_train)\n",
    "        valid_dataset = TensorDataset(X_valid_cats, X_valid_nums, y_valid)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=512)\n",
    "        \n",
    "        model = SurvivalModel(CAT_SIZE, CAT_EMB, len(NUMS)).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        train_model(model, train_loader, valid_loader, optimizer, criterion, device, EPOCHS)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            oof_nn[test_index] += model(X_valid_cats.to(device), X_valid_nums.to(device)).cpu().numpy().flatten()\n",
    "\n",
    "        X_test_cats = torch.tensor(test[CATS].values, dtype=torch.long)\n",
    "        X_test_nums = torch.tensor(test[NUMS].values, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            pred_nn += model(X_test_cats.to(device), X_test_nums.to(device)).cpu().numpy().flatten()\n",
    "\n",
    "oof_nn /= REPEATS\n",
    "pred_nn /= (FOLDS * REPEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "716a01ad-7c3e-4f1a-9a98-db0601a66192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall CV for NN = 0.6500187672623404\n"
     ]
    }
   ],
   "source": [
    "from metric import score\n",
    "\n",
    "y_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_pred = train[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = oof_nn\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(f\"\\nOverall CV for NN =\",m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e04d1-c7dc-4c10-bd58-42aa5c7297f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
